{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Cover5-01.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ тональности текста (Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment analysis (по-русски, анализ тональности)** — это область компьютерной лингвистики, которая занимается изучением мнений и эмоций в текстовых документах. \n",
    "\n",
    "Анализ тональности находит свое практическое применение в разных областях:\n",
    "- социология — собираем данные из соц. сетей (например, о религиозных взглядах)\n",
    "- политология — собираем данные из блогов о политических взглядах населения\n",
    "- маркетинг — анализируем Твиттер, чтобы узнать какая модель ноутбуков пользуется наибольшим спросом\n",
    "- медицина и психология — определяем депрессию у пользователей соц. сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подходы к классификации тональности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ тональности обычно определяют как одну из задач компьютерной лингвистики, т.е. подразумевается, что мы можем найти и классифицировать тональность, используя инструменты обработки естественного языка (такие как теггеры, парсеры и др.). Сделав большое обобщение, можно разделить существующие подходы на следующие категории:\n",
    "\n",
    "<img src='t4.png', align=\"left\", width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Первый тип систем** состоит из набора правил, применяя которые система делает заключение о тональности текста. Например, для предложения «Я люблю кофе», можно применить следующее правило:\n",
    "\n",
    "- если сказуемое (\"люблю\") входит в положительный набор глаголов (\"люблю\", \"обожаю\", \"одобряю\" ...) и в предложении не имеется отрицаний, то классифицировать тональность как \"положительная\"\n",
    "\n",
    "Многие коммерческие системы используют данный подход, несмотря на то что он требует больших затрат, т.к. для хорошей работы системы необходимо составить большое количество правил."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подходы, **основанные на словарях**, используют так называемые тональные словари (affective lexicons) для анализа текста. В простом виде тональный словарь представляет из себя список слов со значением тональности для каждого слова. Вот пример из базы ANEW, переведенный на русский:\n",
    "\n",
    "слово валентность --- (1-9)\n",
    "- счастливый ---\t8.21\n",
    "- хороший\t--- 7.47\n",
    "- скучный\t--- 2.95\n",
    "- сердитый --- 2.85\n",
    "- грустный --- 1.61\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Машинное обучение без учителя** представляет собой, наверное, наиболее интересный и в то же время наименее точный метод анализа тональности. Одним из примеров данного метода может быть автоматическая кластеризация документов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Машинное обучение с учителем** является наиболее распространенным методом, используемым в исследованиях. Его суть состоит в том, чтобы обучить машинный классификатор на коллекции заранее размеченных текстах, а затем использовать полученную модель для анализа новых документов. Именно про этот метод мы и поговорим далее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделать классификатор тональности коротких текстовых сообщений (твитов). Алгоритм должен уметь классифицировать сообщения на два класса: сообщения с положительной эмоциональной окраской и сообщения с отрицательной окраской."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Приблизительно из 225 тысяч размеченных твитов \n",
    "- Для разметки на два класса (положительные и отрицательные)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data = pd.read_csv('data/positive.csv', sep=';', header=None)\n",
    "n_data = pd.read_csv('data/negative.csv', sep=';', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([p_data, n_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[[3, 4]]\n",
    "dataset.columns = ['text', 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Морфолгоический анализатор и предварительная обработка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Морфологический анализатор** для русского языка — программа, которая приводит слово к начальной форме, определяет падеж, находит словоформы. Этот процесс называется **лемматизация**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    # оставляем в предложении только русские буквы (таким образом\n",
    "    # удалим и ссылки, и имена пользователей, и пунктуацию и т.д.)\n",
    "    alph = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "    \n",
    "    cleaned_text = ''\n",
    "    for char in text:\n",
    "        if (char.isalpha() and char[0] in alph) or (char == ' '):\n",
    "            cleaned_text += char\n",
    "        \n",
    "    result = []\n",
    "    for word in cleaned_text.split():\n",
    "        # лемматизируем\n",
    "        result.append(morph.parse(word)[0].normal_form)\n",
    "                              \n",
    "    return ' '.join(result)\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(text_cleaner)\n",
    "\n",
    "dataset.to_csv('data/cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, теперь у нас есть очищенный и подготовленный текст!\n",
    "Теперь требуется построить признаковое описание текста. Для этого мы будем использовать такие инструменты:\n",
    "\n",
    "- CountVectorizer\n",
    "- TfidfVectorizer \n",
    "    \n",
    "###  CountVectorizer\n",
    "    \n",
    "CountVectorizer преобразовывает входной текст в матрицу, значениями которой, являются количества вхождения данного ключа(слова) в текст. Рассмотрим следующий пример.\n",
    "\n",
    "Допустим есть таблица с текстовыми значениями: \n",
    "\n",
    "_________________________\n",
    "\n",
    "раз два три\n",
    "\n",
    "три четыре два два\n",
    "\n",
    "раз раз раз четыре\n",
    "_________________________\n",
    "\n",
    "\n",
    "\n",
    "Для начала CountVectorizer собирает уникальные ключи из всех записей, в нашем примере это будет:\n",
    "\n",
    "[раз, два, три, четыре]\n",
    "\n",
    "Длина списка из уникальных ключей и будет длиной нашего закодированного текста (в нашем случае это 4). А номера элементов будут соответствовать, количеству раз встречи данного ключа с данным номером в строке:\n",
    "\n",
    "раз два три --> [1,1,1,0]\n",
    "\n",
    "три четыре два два --> [0,2,1,1]\n",
    "\n",
    "Соответственно после кодировки, применения данного метода мы получим:\n",
    "\n",
    "_________________________\n",
    "\n",
    "1,1,1,0\n",
    "\n",
    "0,2,1,1\n",
    "\n",
    "3,0,0,1\n",
    "_________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "\n",
    "TfidfVectorizer действует похожим образом, но вместо количества вхождения слова в текст считает TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF** (от англ. TF — term frequency, IDF — inverse document frequency) — статистическая мера, используемая для оценки важности слова в контексте документа, являющегося частью коллекции документов или корпуса. Вес некоторого слова пропорционален количеству употребления этого слова в документе, и обратно пропорционален частоте употребления слова в других документах коллекции.\n",
    "\n",
    "**TF** (term frequency — частота слова) — отношение числа вхождений некоторого слова к общему числу слов документа. Таким образом, оценивается важность слова \n",
    "$t_{{i}}$ в пределах отдельного документа.\n",
    "\n",
    "\n",
    "$TF(t,d)=\\frac {n_{t}}{\\sum _{k}n_{k}}$\n",
    "\n",
    "\n",
    "где $n_t$ есть число вхождений слова \n",
    "$t$ в документ, а в знаменателе — общее число слов в данном документе.\n",
    "\n",
    "IDF (inverse document frequency — обратная частота документа) — инверсия частоты, с которой некоторое слово встречается в документах коллекции. Основоположником данной концепции является Карен Спарк Джонс[1]. Учёт IDF уменьшает вес широкоупотребительных слов. Для каждого уникального слова в пределах конкретной коллекции документов существует только одно значение IDF.\n",
    "\n",
    "$IDF(t,D)=\\log {\\frac {|D|}{|\\{\\,d_{i}\\in D\\mid t\\in d_{i}\\,\\}|}}$\n",
    "\n",
    "где\n",
    "\n",
    "$|D|$ — число документов в коллекции;\n",
    "\n",
    "$|\\{\\,d_{i}\\in D\\mid t\\in d_{i}\\,\\}|$ — число документов из коллекции $D$, в которых встречается \n",
    "$t$ (когда $n_{t}\\neq 0$).\n",
    "\n",
    "Выбор основания логарифма в формуле не имеет значения, поскольку изменение основания приводит к изменению веса каждого слова на постоянный множитель, что не влияет на соотношение весов.\n",
    "\n",
    "Таким образом, мера TF-IDF является произведением двух сомножителей: $TF\\times IDF$\n",
    "\n",
    "Большой вес в TF-IDF получат слова с высокой частотой в пределах конкретного документа и с низкой частотой употреблений в других документах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# считываем подготовленный датасет\n",
    "dataset = pd.read_csv('data/cleaned_data.csv', index_col=0).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-граммная схема, которая будут использоваться в работе\n",
    "# например, (1, 3) означает униграммы + биграммы + триграммы\n",
    "ngram_scheme = (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram Scheme: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "print('N-gram Scheme:', ngram_scheme)\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer = \"word\", ngram_range=ngram_scheme) \n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer = \"word\", ngram_range=ngram_scheme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flaren/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/flaren/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import ShuffleSplit, cross_val_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь запустим классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 0.7364046495907359\n",
      "Linear: 0.7414921600611324\n",
      "Linear Parameters: {'alpha': 0.0001, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = count_vectorizer\n",
    "\n",
    "X = vectorizer.fit_transform(dataset['text'])\n",
    "y = dataset['label']\n",
    "\n",
    "cv = ShuffleSplit(len(y), n_iter=5, test_size=0.3, random_state=0)\n",
    "\n",
    "# наивный байес\n",
    "clf = MultinomialNB()\n",
    "NB_result = cross_val_score(clf, X, y, cv=cv).mean()\n",
    "\n",
    "# линейный классификатор\n",
    "clf = SGDClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'loss': ('log', 'hinge'),\n",
    "    'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "    'alpha': [0.001, 0.0001, 0.00001, 0.000001]\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(clf, parameters, cv=cv, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X, y)\n",
    "\n",
    "L_result = gs_clf.best_score_\n",
    "\n",
    "print('NB:', NB_result.mean())\n",
    "print('Linear:', L_result)\n",
    "print('Linear Parameters:', gs_clf.best_params_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Используя предложенные выше инструменты, попытаться повысить качество классификации\n",
    "- Верно ли, что чем больше n в n-граммных схемах, тем точнее работает алгоритм? Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылки на источники:\n",
    "\n",
    "- https://habr.com/post/49421/\n",
    "- https://habr.com/post/205360/\n",
    "- Ю. В. Рубцова. Построение корпуса текстов для настройки тонового классификатора // Программные продукты и системы, 2015, №1(109), –С.72-78"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
